This is the log for the project


November 23rd: We finally get the green light from the lecturer to start working on the project.

Workday 1 (November 25th): We start by defining the flow from the start till the end. First the program will be a "layer" over the screen for the pen to work on, the layer can be only interacted by the pen. The camera will identify the screen and start tracking the pen which will have a red dot on top of it for the camera to easily track it, However since the lecturer said it should use only one camera a problem occures, we can't determain the ditance of the pen from the "virtual board", if we're using only one camera we need an object with known width, height etc and since projector ditance from board can vary the screen isn't really reliable for calibration. Another problem popped up is that the opencv match template doesn't seem to be able to find complex templates, while it was able to identify my terminal in screen (screenshot of the terminal as template and screenshot of screen containing the template) it wasn't able to identify Malik's photo as template, since we don't really know what's gonna be on the screen the matchtemplate function isn't really reliable. A solution we thought of is that on app start the screen shows a simple predefined object in fullscreen mode which the camera detects then we can know the coordinations of the projected screen, even though it will need recalibration on camera move it's the best we got for now. Yet another problem popped up is that we also need to find the degree of the pen for smooth wrighting experience since not all people hold it at 90 degree.

Workday 2 (November 29th): After talking with Dan Feldman my big data professor and head of the robotics and big data lab and Ernesto Sanches a good friend of mine and a doctorandus of Dan's they explained that a calibration cannot be done without an object with pre known dimensions. However they also gave me something that may solve all my problems, calibration, screen distance, pen distance, pen degree and my back problems, aruco.  After testing it seems that it's really easy to detect those markers, however, I tested the detection with aruco photo on the phone and it wasn't able to detect it from distance, putting a small one on the pen doesn't look like it's going to work, if it can it will easily give the location of the pen and its angel which will make it easy to tell when the pen reaches the board. Further testing is required, with lower aruco marker resolution and it needs to be printed. The flow seems a bit clearer now after reading more about opencv especially opencv4, I'm thinking of taking each video frame and locate the aruco marker on the pen if possible (if not then red dot), check if it's close to the board and if it is then put a colored circle on the location (when done 30 times a second it will appear as a line). One thing might hinder this is the computation speed, will there be an increasing latency between the person's movement and the screen writing? we'll see.
